{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segregate Train and test data from face_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from random import shuffle\n",
    "import shutil\n",
    "import math\n",
    "test_dir = \"blue_cis6930/nghosh/Test\"\n",
    "train_dir = \"blue_cis6930/nghosh/Train\"\n",
    "input_dir = \"face_images\"\n",
    "if not os.path.isdir(\"blue_cis6930\"):\n",
    "    os.mkdir(\"blue_cis6930\")\n",
    "    os.mkdir(\"blue_cis6930/nghosh\")\n",
    "\n",
    "if not os.path.isdir(test_dir) and not os.path.isdir(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(test_dir)\n",
    "inside_test = os.listdir(test_dir)\n",
    "inside_train = os.listdir(train_dir)\n",
    "if len(inside_test) == 0 or len(inside_train) == 0:\n",
    "    files = [os.path.join(input_dir,fle) for fle in os.listdir(input_dir)]\n",
    "    dst_dir = train_dir\n",
    "    file_count = sum([len(files) for r, d, files in os.walk(input_dir)])\n",
    "    train_num = math.ceil(0.9*float(file_count))\n",
    "    test_num = file_count - train_num\n",
    "    num_copied = 0\n",
    "    shuffle(files)\n",
    "    for fle in files:\n",
    "      if os.path.isfile(fle):\n",
    "          with open(fle) as f:\n",
    "            if num_copied == train_num:\n",
    "              dst_dir = test_dir\n",
    "            shutil.copy(fle, dst_dir)\n",
    "            num_copied += 1\n",
    "    print(\"Data Segregated successfully!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES4Zy1uEUMyv"
   },
   "source": [
    "# Data Loading & augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4565,
     "status": "ok",
     "timestamp": 1605326648045,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "lQ0k9hC-40RO"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eee9427d6a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageMath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/site-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlsun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSUN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSUNClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoCaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCIFAR100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstl10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/site-packages/torchvision/datasets/lsun.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/site-packages/PIL/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ;-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mVERSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.1.7'\u001b[0m  \u001b[0;31m# PIL Version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/apps/python3/3.6.5/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageMath\n",
    "import glob\n",
    "import cv2\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBC3ZNEaRHyN"
   },
   "outputs": [],
   "source": [
    "# Define device for Cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1605326654632,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "D0q42pN0Wscu"
   },
   "outputs": [],
   "source": [
    "# Custom class RandomScale to randomly scale the values of RGB image between 0.6, 1\n",
    "# Used for data augmentation\n",
    "\n",
    "class RandomScale(object):\n",
    "    def __call__(self, img):\n",
    "        rand = torch.tensor(random.uniform(0.6,1.0), dtype=torch.float32)\n",
    "        img = (torchvision.transforms.ToTensor()(img)).float()\n",
    "        img = img*rand\n",
    "        return img\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1605326672054,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "B4y0_xWanQY_"
   },
   "outputs": [],
   "source": [
    "# Utility function to convert RGB to lab image\n",
    "# Post conversion, returns L and mean values for a & b channels\n",
    "def convertRGBToLAB(img):\n",
    "  lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB).astype(\"float32\")\n",
    "  l,a,b = cv2.split(lab)\n",
    "  return (l, np.mean(a), np.mean(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1605326675117,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "QeAcKp3_GRAj"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset that applies random scale, random horizontal flip and\n",
    "# random crop to each train image 10 times\n",
    "# Resize each augmented image to 128x128\n",
    "\n",
    "class ColorDataset(Dataset):\n",
    "  def __init__(self, folder_path, transform=None):\n",
    "    self.image_list = glob.glob(folder_path)\n",
    "    self.transform = transform\n",
    "    self.T = []\n",
    "    for img in self.image_list:\n",
    "      single_img = Image.open(img)\n",
    "      \n",
    "      if self.transform !=None:\n",
    "        for i in range(10):\n",
    "          tran = self.transform(single_img)\n",
    "          l,a,b = convertRGBToLAB(np.asarray(tran.permute(1,2,0)))\n",
    "          l = l/100\n",
    "          self.T.append((l,a,b))\n",
    "    self.data_len = len(self.T)\n",
    "       \n",
    "  def __getitem__(self, index):\n",
    "    return self.T[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 153336,
     "status": "ok",
     "timestamp": 1605326832847,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "2RMMKvlP4uX5"
   },
   "outputs": [],
   "source": [
    "# Load the train dataset after creating 10 augmented & resized image per input train image\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomResizedCrop(128),\n",
    "    RandomScale(),\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "train_dir = \"blue_cis6930/nghosh/Train/*\"\n",
    "dataset = ColorDataset(train_dir, transforms)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, shuffle=True, batch_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtQkUH_IUXLv"
   },
   "source": [
    "# Network definition & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1605326923768,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "rUUyz0gDdJnS"
   },
   "outputs": [],
   "source": [
    "# Network Defination for the Simple regressor\n",
    "\n",
    "class RegNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(RegNetwork, self).__init__()    \n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(1,3,kernel_size=3,stride=2,padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(3,3,kernel_size=3,stride=2,padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(3,3,kernel_size=3,stride=2,padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(3,3,kernel_size=3,stride=2,padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(3,3,kernel_size=3,stride=2,padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(3,3,kernel_size=3,stride=2,padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(3,2,kernel_size=3,stride=2,padding=1)\n",
    "    )\n",
    "  def forward(self, inputVal):\n",
    "      inputVal = self.layers(inputVal)       \n",
    "      return inputVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLmXUEWMfppY"
   },
   "outputs": [],
   "source": [
    "# Initialize the model as the simple regressor network\n",
    "model = RegNetwork().to(device)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1605326962682,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "TV2rYZ-mpudJ"
   },
   "outputs": [],
   "source": [
    "# Function to train the network\n",
    "def train(grayImage, targetVals, criterion, optimizer, model):\n",
    "  predValues = model(grayImage)\n",
    "  loss_train = criterion(predValues, targetVals)\n",
    "  optimizer.zero_grad()\n",
    "  loss_train.backward()\n",
    "  optimizer.step()\n",
    "  return loss_train.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 100631,
     "status": "ok",
     "timestamp": 1605327073200,
     "user": {
      "displayName": "Disha Nayar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiS69nbJLFGghEpTlSKDC3IYIEILqNx6ZV7OVI5SA=s64",
      "userId": "03006421350416810084"
     },
     "user_tz": 300
    },
    "id": "zf79eN8SrTQe",
    "outputId": "fd41a0b3-2305-44d6-ca11-25a8f856c9f6"
   },
   "outputs": [],
   "source": [
    "# Train the network for 200 epochs \n",
    "# Display the graph for training loss error\n",
    "loss_list = []\n",
    "for epoch in range(200):\n",
    "  totalLoss = 0\n",
    "  avgLoss = 0\n",
    "  for gImage,a,b in train_loader:\n",
    "    gImage = torch.unsqueeze(gImage, 1)\n",
    "    targetVals = torch.empty(a.shape[0], 2, 1, 1)\n",
    "    targetVals[:,0, :, :] = torch.unsqueeze(torch.unsqueeze(a, -1), -2)\n",
    "    targetVals[:,1, :, :] = torch.unsqueeze(torch.unsqueeze(b, -1), -2)\n",
    "    if torch.cuda.is_available():\n",
    "      gImage = gImage.to(device)\n",
    "      targetVals = targetVals.to(device)\n",
    "    totalLoss = totalLoss + train(gImage,targetVals, criterion, optimizer,model)\n",
    "  avgLoss = totalLoss / len(train_loader)\n",
    "  loss_list.append(avgLoss)\n",
    "  print(\"Loss in epoch \", epoch, \" is \", avgLoss)\n",
    "plt.plot(np.array(loss_list), 'r')\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6. (full)",
   "language": "python",
   "name": "python3-3.6-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
